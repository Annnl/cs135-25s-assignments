{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import textwrap\n",
    "import re \n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split,RandomizedSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\n",
    "from sklearn.metrics import make_scorer, roc_auc_score ,confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train_df: (5557, 32)\n",
      "Shape of y_train_df: (5557, 6)\n",
      "Index(['author', 'title', 'passage_id', 'text', 'char_count', 'word_count',\n",
      "       'sentence_count', 'avg_word_length', 'avg_sentence_length',\n",
      "       'type_token_ratio', 'pronoun_freq', 'function_words_count',\n",
      "       'punctuation_frequency', 'sentiment_polarity', 'sentiment_subjectivity',\n",
      "       'readability_Kincaid', 'readability_ARI', 'readability_Coleman-Liau',\n",
      "       'readability_FleschReadingEase', 'readability_GunningFogIndex',\n",
      "       'readability_LIX', 'readability_SMOGIndex', 'readability_RIX',\n",
      "       'readability_DaleChallIndex', 'info_characters_per_word',\n",
      "       'info_syll_per_word', 'info_words_per_sentence',\n",
      "       'info_type_token_ratio', 'info_characters', 'info_syllables',\n",
      "       'info_words', 'info_wordtypes'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = 'data_readinglevel'\n",
    "x_train_df = pd.read_csv(os.path.join(data_dir, 'x_train.csv'))\n",
    "y_train_df = pd.read_csv(os.path.join(data_dir, 'y_train.csv'))\n",
    "y_train_df['stage_encoded'] = y_train_df['Coarse Label'].map({'Key Stage 2-3': 0, 'Key Stage 4-5': 1})\n",
    "y_train_clean = y_train_df['stage_encoded'].values\n",
    "N, n_cols = x_train_df.shape\n",
    "print(\"Shape of x_train_df: (%d, %d)\" % (N, n_cols))\n",
    "print(\"Shape of y_train_df: %s\" % str(y_train_df.shape))\n",
    "print(x_train_df.columns)\n",
    "def load_arr_from_npz(npz_path):\n",
    "    ''' Load array from npz compressed file given path\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr : numpy ndarray\n",
    "    '''\n",
    "    npz_file_obj = np.load(npz_path)\n",
    "    arr = npz_file_obj.f.arr_0.copy() # Rely on default name from np.savez\n",
    "    npz_file_obj.close()\n",
    "    return arr\n",
    "xBERT_train_NH = load_arr_from_npz(os.path.join(\n",
    "        data_dir, 'x_train_BERT_embeddings.npz'))\n",
    "len(xBERT_train_NH) == len(x_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text_list = x_train_df['text'].values\n",
    "def custom_tokenizer(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "random_state=1543\n",
    "x_train_df = x_train_df.drop(['author', 'title', 'passage_id','text'], axis=1)\n",
    "y_train_clean = np.array(y_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['author', 'title', 'passage_id', 'text', 'char_count', 'word_count',\n",
      "       'sentence_count', 'avg_word_length', 'avg_sentence_length',\n",
      "       'type_token_ratio', 'pronoun_freq', 'function_words_count',\n",
      "       'punctuation_frequency', 'sentiment_polarity', 'sentiment_subjectivity',\n",
      "       'readability_Kincaid', 'readability_ARI', 'readability_Coleman-Liau',\n",
      "       'readability_FleschReadingEase', 'readability_GunningFogIndex',\n",
      "       'readability_LIX', 'readability_SMOGIndex', 'readability_RIX',\n",
      "       'readability_DaleChallIndex', 'info_characters_per_word',\n",
      "       'info_syll_per_word', 'info_words_per_sentence',\n",
      "       'info_type_token_ratio', 'info_characters', 'info_syllables',\n",
      "       'info_words', 'info_wordtypes'],\n",
      "      dtype='object')\n",
      "<class 'numpy.ndarray'>\n",
      "One-hot encoded shape: (5557, 439)\n",
      "Bert array has shape: (5557, 768)\n",
      "x_train array has shape: (5557, 28)\n",
      "Final feature matrix shape: (5557, 1235)\n"
     ]
    }
   ],
   "source": [
    "#tr_text_list = x_train_df['text'].values\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "random_state=1543\n",
    "\n",
    "#x_train_df = x_train_df.drop(['author', 'title', 'passage_id','text'], axis=1)\n",
    "# x_train_df = pd.concat([\n",
    "#     pd.DataFrame(xBERT_train_NH, columns=['bert']),\n",
    "#     x_train_df\n",
    "# ], axis=1)\n",
    "\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = ['author', 'title']\n",
    "print(x_train_df.columns)\n",
    "# One-Hot Encode 'author' and 'title'\n",
    "categorical_data = x_train_df.loc[:, categorical_cols]  # Explicitly select columns as DataFrame\n",
    "\n",
    "# One-Hot Encode 'author' and 'title'\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "one_hot_encoded = encoder.fit_transform(categorical_data)\n",
    "one_hot_encoded = one_hot_encoded.toarray()\n",
    "\n",
    "print(type(one_hot_encoded))\n",
    "print(f\"One-hot encoded shape: {one_hot_encoded.shape}\")\n",
    "\n",
    "\n",
    "x_train_df = x_train_df.drop(columns=['author','title','passage_id','text'])\n",
    "\n",
    "# Convert BERT embeddings to DataFrame\n",
    "bert_df = pd.DataFrame(xBERT_train_NH, columns=[f'bert_{i}' for i in range(xBERT_train_NH.shape[1])])\n",
    "bert_array = bert_df.to_numpy()\n",
    "print(f\"Bert array has shape: {bert_array.shape }\")\n",
    "print(f\"x_train array has shape: {x_train_df.to_numpy().shape }\")\n",
    "\n",
    "# Concatenate all features\n",
    "X_train_final_np = np.hstack((one_hot_encoded,x_train_df.to_numpy(),bert_array))\n",
    "\n",
    "# Print final shape\n",
    "print(f\"Final feature matrix shape: {X_train_final_np.shape}\")\n",
    "\n",
    "y_train_clean = np.array(y_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = X_train_final_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m      1\u001b[0m numeric_transformer \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m, FunctionTransformer(validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)) \n\u001b[0;32m      3\u001b[0m ])\n\u001b[0;32m      6\u001b[0m text_transformer \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      7\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf\u001b[39m\u001b[38;5;124m'\u001b[39m, TfidfVectorizer(\n\u001b[0;32m      8\u001b[0m         min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, MaxAbsScaler())\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     18\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m ColumnTransformer([\n\u001b[1;32m---> 19\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m'\u001b[39m, numeric_transformer, x_train_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()),\n\u001b[0;32m     20\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, text_transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m     21\u001b[0m ], remainder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdrop\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m full_pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     24\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[0;32m     25\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m, MLPClassifier(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m         random_state\u001b[38;5;241m=\u001b[39mrandom_state))\n\u001b[0;32m     31\u001b[0m ])\n\u001b[0;32m     34\u001b[0m my_parameter_grid_by_name \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprep__text__tfidf__min_df\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m50\u001b[39m] ,\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprep__text__tfidf__max_df\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.7\u001b[39m,\u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m0.85\u001b[39m]  ,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass__batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m]\n\u001b[0;32m     49\u001b[0m }\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('passthrough', FunctionTransformer(validate=False)) \n",
    "])\n",
    "\n",
    "\n",
    "text_transformer = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=1,\n",
    "        max_df=1,\n",
    "        ngram_range=(1,1),\n",
    "        stop_words=None,\n",
    "        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "        lowercase=False)),\n",
    "    ('scaler', MaxAbsScaler())\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', numeric_transformer, x_train_df.columns.tolist()),\n",
    "    ('text', text_transformer, 'text') \n",
    "], remainder='drop')\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('class', MLPClassifier(\n",
    "        max_iter=200,\n",
    "        early_stopping=True,\n",
    "        learning_rate='adaptive',\n",
    "        solver='sgd',\n",
    "        random_state=random_state))\n",
    "])\n",
    "\n",
    "\n",
    "my_parameter_grid_by_name = {\n",
    "    'prep__text__tfidf__min_df': [1,2,5,10,30,50] ,\n",
    "    'prep__text__tfidf__max_df': [0.7,0.75, 0.8,0.85]  ,\n",
    "    'prep__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "    'prep__text__tfidf__stop_words': [None, 'english'],\n",
    "    'prep__text__tfidf__token_pattern': [\n",
    "        r'(?u)\\b[\\w-]+\\b',\n",
    "        r'(?u)\\b\\w\\w+\\b',\n",
    "        r'(?u)\\b(?!\\d+\\b)\\w+\\b'],\n",
    "    'prep__text__tfidf__lowercase': [False, True],\n",
    "    'class__hidden_layer_sizes': [(64,), (128,)],\n",
    "    'class__activation': ['relu', 'tanh','logistic'],\n",
    "    'class__alpha': [0.001, 0.01],\n",
    "    'class__learning_rate_init': [0.001, 0.01],\n",
    "    'class__batch_size': [32, 64]\n",
    "}\n",
    "\n",
    "my_scoring_metric_name = 'roc_auc'\n",
    "X_combined = pd.concat([\n",
    "    pd.DataFrame(tr_text_list, columns=['text']),\n",
    "    x_train_df\n",
    "], axis=1)\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_combined, y_train_clean, test_size=0.1, random_state=1543, stratify=y_train_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ningn\\anaconda3\\envs\\cs135_25s_env\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:697: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_iter = [50]\n",
    "# valid_indicators_N = np.zeros(N)\n",
    "# valid_indicators_N[valid_ids] = -1\n",
    "# my_splitter = sklearn.model_selection.PredefinedSplit(valid_indicators_N)\n",
    "y_train_clean = list(y_train_clean)\n",
    "my_best_models = []\n",
    "best_test_scores = []\n",
    "for iter in n_iter:\n",
    "    random_searcher = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    my_parameter_grid_by_name,\n",
    "    scoring=my_scoring_metric_name ,\n",
    "    cv=sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state),\n",
    "    refit=True,\n",
    "    n_iter=iter,  \n",
    "    random_state=random_state, \n",
    "    return_train_score=True \n",
    "    )\n",
    "    random_searcher.fit(X_train_val, y_train_val)\n",
    "    # Get the best-trained model (automatically refit on full training data)\n",
    "    best_model = random_searcher.best_estimator_\n",
    "    my_best_models.append(best_model)\n",
    "\n",
    "    # Best hyperparameters\n",
    "    best_params = random_searcher.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Best cross-validation score (average accuracy across folds)\n",
    "    best_score = random_searcher.best_score_\n",
    "    print(f\"Best CV AUROC: for {iter} iterations is {best_score:.4f}\")\n",
    "\n",
    "    #use best model to get predictions on test data\n",
    "    y_test_probs = best_model.predict_proba(X_test)[:, 1]  \n",
    "\n",
    "    # Compute ROC AUC score\n",
    "    roc_auc = roc_auc_score(y_test, y_test_probs)\n",
    "    best_test_scores.append(roc_auc)\n",
    "    predicted_labels = np.where(y_test_probs >= 0.5, 1, 0)\n",
    "    cm = confusion_matrix(y_test,predicted_labels )\n",
    "    cm_df = pd.DataFrame(cm, \n",
    "                     index=['Actual 0', 'Actual 1'], \n",
    "                     columns=['Predicted 0', 'Predicted 1'])\n",
    "\n",
    "    print(cm_df)\n",
    "    print(f\"Test Set ROC AUC Score for {iter}:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_iter[np.argmax(best_test_scores)])\n",
    "best_model_overall = my_best_models[np.argmax(best_test_scores)]\n",
    "print(best_model_overall.get_params())\n",
    "#print(y_test.shape)\n",
    "\n",
    "overall_train_roc = roc_auc_score(y_train_clean,best_model_overall.predict_proba(tr_text_list)[:,1])\n",
    "\n",
    "print(overall_train_roc)\n",
    "x_test_df = pd.read_csv(os.path.join(data_dir, 'x_test.csv'))\n",
    "tr_test_list = x_test_df['text'].values.tolist()\n",
    "#print(tr_test_list[:5])\n",
    "yproba_N2 = best_model_overall.predict_proba(tr_test_list)\n",
    "print(yproba_N2[:5])\n",
    "y_proba_N1 = yproba_N2[:,1]\n",
    "print(y_proba_N1[:5])\n",
    "print(y_proba_N1.shape)\n",
    "np.savetxt(\"yproba1_test.txt\", y_proba_N1, fmt=\"%.6f\")\n",
    "loaded_probs = np.loadtxt('yproba1_test.txt')\n",
    "print(loaded_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(random_searcher.cv_results_)\n",
    "\n",
    "param_columns = [col for col in results_df.columns if col.startswith(\"param_\")]\n",
    "for col in param_columns:\n",
    "\n",
    "    results_df[col] = results_df[col].apply(\n",
    "        lambda x: 'None' if x is None else str(x) \n",
    "    )\n",
    "\n",
    "results_df = results_df[param_columns + [\"mean_test_score\"]]\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_25s_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
